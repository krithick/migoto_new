# Project Status: Dynamic Persona System

## üì¶ What We've Completed So Far

### ‚úÖ Part 1: Persona Generation System (DONE)

**What we designed:**
- Dynamic detail category selection (LLM chooses what's relevant)
- Rich persona generation with scenario-specific details
- Natural prompt generation (generate, not fill)
- Safe implementation with fallback logic

**Documents created:**
1. `IMPLEMENTATION_GUIDE_FOR_AMAZON_Q.md` - Full implementation
2. `ARCHITECTURE_OVERVIEW.md` - System design explanation
3. `SAFE_IMPLEMENTATION_GUIDE.md` - Safe, non-breaking implementation

**Key Features:**
- ‚úÖ LLM analyzes scenario and picks 3-8 relevant detail categories
- ‚úÖ 15+ detail categories in library (professional_context, family_context, etc.)
- ‚úÖ PersonaInstance model (base fields + dynamic details)
- ‚úÖ PersonaGenerator class with parallel generation
- ‚úÖ PromptGenerator creates natural prompts from persona data
- ‚úÖ Safe v2 functions with fallback to v1
- ‚úÖ Feature flags for gradual rollout

---

## ‚ùå What's Still NOT Done

### Part 2: Extraction Improvements (TODO)

**Current issue:** Extraction (`extract_scenario_info`) doesn't capture enough context

**What needs improvement:**
1. Better mode descriptions (what happens in learn/assess/try modes)
2. Extract persona types (not just persona details)
3. Extract coaching rules for try mode
4. Extract evaluation criteria
5. Identify which detail categories the scenario needs
6. Extract domain knowledge more thoroughly

**Key questions:**
- How to extract "persona types" vs "persona instances"?
- How to identify what detail categories a scenario needs?
- How to extract coaching rules from documents?
- How to structure domain_knowledge in template_data?

### Part 3: Prompt Architecture (TODO)

**What's needed:**
1. Define prompt sections clearly
2. Archetype-specific prompt templates
3. Guardrails and behavioral rules
4. Handle off-topic questions properly
5. Conversation flow management
6. First response instructions
7. Closing conditions

**Key questions:**
- What sections should every prompt have?
- How do archetypes affect prompt structure?
- Where do guardrails go in the prompt?
- How to handle "DO NOT" rules consistently?

### Part 4: Learn Mode (TODO)

**What's needed:**
1. Learn mode persona generation (different from assess mode)
2. Learn mode prompt structure (teaching focused)
3. How learn mode uses domain_knowledge
4. Connection with vector DB for facts
5. Teaching methodology integration

**Key questions:**
- Should learn mode have persona variations?
- How different should learn mode prompts be?
- How does it connect to vector DB?

### Part 5: Try Mode & Coaching (TODO)

**What's needed:**
1. When does coach appear?
2. What does coach say?
3. Coaching rules from domain_knowledge
4. Coach personality/tone
5. How coach references methodology

**Key questions:**
- Does coach need a separate persona?
- How does coach know when to intervene?
- Should coaching be inline or separate messages?

### Part 6: Evaluation System (TODO)

**What's needed:**
1. Evaluation metrics structure
2. How to evaluate conversations
3. Scoring methodology
4. Success/failure indicators
5. Output format for results

**Key questions:**
- When does evaluation happen?
- Who does the evaluation (separate LLM call)?
- What data does evaluator need?

### Part 7: Full Integration (TODO)

**What's needed:**
1. End-to-end API flow
2. How all pieces connect
3. Database schema updates
4. Frontend integration points
5. Comprehensive testing

---

## üìã Recommended Order for Remaining Work

### Priority 1: Extraction Improvements (NEXT)
Why first? Everything else depends on good extraction.

**What to focus on:**
1. Extract persona TYPES (not just instances)
2. Extract mode descriptions clearly
3. Extract domain_knowledge structure
4. Identify required detail categories hint

### Priority 2: Prompt Architecture
Why second? Need to know how prompts work before generating them.

**What to focus on:**
1. Define prompt sections
2. Create archetype-specific templates
3. Add strong guardrails
4. Handle edge cases

### Priority 3: Learn Mode
Why third? Simpler than try mode, good to validate system.

**What to focus on:**
1. Learn mode persona generation
2. Learn mode prompts
3. Teaching methodology

### Priority 4: Try Mode & Coaching
Why fourth? Builds on assess mode + adds coaching layer.

**What to focus on:**
1. Coaching trigger logic
2. Coach messages
3. Integration with assess mode

### Priority 5: Evaluation
Why fifth? Happens after conversation, can be added last.

**What to focus on:**
1. Evaluation criteria
2. Scoring logic
3. Output format

### Priority 6: Full Integration
Why last? Connects everything together.

**What to focus on:**
1. API flow
2. Testing
3. Documentation

---

## üéØ What to Work On Next

**Immediate next step: Extraction Improvements**

### Quick Context:

**Current Extraction** (simplified):
```python
template_data = {
    "general_info": {...},
    "persona_definitions": {
        "assess_mode_ai_bot": {
            "name": "Dr. Archana",
            "role": "Gynecologist",
            ...
        }
    },
    "domain_knowledge": {...}
}
```

**What's Missing:**
1. No "persona_types" (only specific instances)
2. No clear mode descriptions
3. No coaching rules extraction
4. No evaluation criteria
5. Domain knowledge not structured well

**What We Need:**
```python
template_data = {
    "title": "...",
    "description": "...",
    
    # NEW: Clear mode descriptions
    "learn_mode": {
        "what_happens": "...",
        "ai_bot_role": "...",
        "learner_role": "...",
        "teaching_focus": "...",
        "methods": ["IMPACT"]
    },
    
    "assess_mode": {
        "what_happens": "...",
        "learner_role": "...",
        "context": "...",
        
        # NEW: Persona TYPES (not instances)
        "persona_types": [
            {
                "type": "Experienced Gynecologist",
                "description": "...",
                "use_case": "...",
                "key_characteristics": {...}
            }
        ]
    },
    
    # NEW: Better structured domain knowledge
    "domain_knowledge": {
        "methodology": "IMPACT",
        "methodology_steps": [...],
        "key_facts": [...],
        "evaluation_criteria": {...},
        "coaching_rules": {...}
    }
}
```

---

## ü§î Questions Before Proceeding

### About Extraction:

1. **Should extraction use LLM** to identify persona types and detail categories?
   - Or should we have rules/patterns?

2. **For documents without explicit personas** - should extraction create persona type descriptions?
   - Example: Doc says "pitch to a doctor" but no specific doctor mentioned

3. **How to extract coaching rules** when they're narrative text?
   - Example: "Learner should acknowledge intent..." ‚Üí How to structure?

4. **Domain knowledge** - what's the complete structure needed?
   - What fields must always be there?
   - What's optional?

### About System Integration:

1. **Should we do extraction improvements BEFORE Amazon Q implements persona generation?**
   - Or can they happen in parallel?

2. **Do you want to test persona generation first** with current extraction?
   - See if it works with existing template_data?

3. **Are there example scenario documents** I should reference?
   - Would help design extraction better

---

## üí° Recommendation

**My suggestion for next steps:**

1. **Test what we have** - Have Amazon Q implement persona generation v2
   - See if it works with CURRENT extraction
   - Identify what's missing in practice

2. **Based on results** - Design extraction improvements
   - We'll know exactly what extraction needs to provide

3. **Then continue** with prompt architecture, modes, etc.

**OR**

1. **Design extraction improvements now** (before implementation)
   - Make sure extraction provides everything persona generation needs
   - Then implement both together

**Which approach do you prefer?** üéØ

---

## üìû Summary

**What's Ready:**
- ‚úÖ Persona Generation System (fully designed, ready to implement)
- ‚úÖ Safe implementation guide (won't break existing code)
- ‚úÖ All code examples provided

**What's Needed Next:**
- ‚ùå Extraction improvements
- ‚ùå Prompt architecture details
- ‚ùå Learn/Try mode specifics
- ‚ùå Evaluation system
- ‚ùå Full integration

**Ready to proceed with:** Whatever you choose next! üöÄ

**Questions to answer:**
1. Should Amazon Q implement persona generation now?
2. Or should we design extraction improvements first?
3. What scenario documents do you have for reference?
