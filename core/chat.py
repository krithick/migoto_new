"""
Enhanced Chat Routes for FastAPI

Routes for handling chat functionality with dynamic avatar interactions and personas.
"""

from fastapi import APIRouter, Depends, HTTPException, status, Body, Form, Query
from fastapi.responses import StreamingResponse
from typing import List, Optional, Dict, Any
from uuid import UUID, uuid4
from datetime import datetime
import json
import re

# Import models and dependencies
from models_old import Message, ChatSession, ChatResponse
from database import get_db
from models.user_models import UserDB , UserRole
from core.user import get_current_user
# Import the new dynamic chat loader system
from dynamic_chat import (
    get_chat_factory,
    initialize_chat_session,
    get_chat_session,
    update_chat_session
)

# Create router
router = APIRouter(tags=["Chat"])

# Chat Routes
@router.post("/chat/initialize")
async def initialize_chat(
    avatar_interaction_id: UUID = Form(...),
    mode: str = Form(...),  # "learn_mode", "try_mode", or "assess_mode"
    persona_id: Optional[UUID] = Form(None),
    avatar_id: Optional[UUID] = Form(None),
    language_id: Optional[UUID] = Form(None),
    db: Any = Depends(get_db),
    current_user: UserDB = Depends(get_current_user)  # Add this dependency
):
    """
    Initialize a new chat session for a specific avatar interaction and mode.
    
    - avatar_interaction_id: ID of the avatar interaction to use
    - mode: Which mode to use (learn_mode, try_mode, assess_mode)
    - persona_id: Optional persona to use for this session
    - language_id: Optional language settings to use
    """
    # Validate mode
    if mode not in ["learn_mode", "try_mode", "assess_mode"]:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid mode. Must be 'learn_mode', 'try_mode', or 'assess_mode'"
        )
    
    # If user is not admin/superadmin, check for assignment
    if current_user.role == UserRole.USER:
        # Find the scenario this avatar interaction belongs to
        scenario = None
        # print(current_user)
        # Check learn mode
        learn_mode_scenarios = await db.scenarios.find({
            "learn_mode.avatar_interaction": str(avatar_interaction_id)
        }).to_list(length=1)
        if learn_mode_scenarios:
            scenario = learn_mode_scenarios[0]
            
        # Check try mode if not found
        if not scenario:
            try_mode_scenarios = await db.scenarios.find({
                "try_mode.avatar_interaction": str(avatar_interaction_id)
            }).to_list(length=1)
            if try_mode_scenarios:
                scenario = try_mode_scenarios[0]
        
        # Check assess mode if not found
        if not scenario:
            assess_mode_scenarios = await db.scenarios.find({
                "assess_mode.avatar_interaction": str(avatar_interaction_id)
            }).to_list(length=1)
            if assess_mode_scenarios:
                scenario = assess_mode_scenarios[0]
        
        # If scenario found, check if user is assigned
        if scenario:
            scenario_id = scenario["_id"]
            
            # Check if scenario is assigned to user
            assignment = await db.user_scenario_assignments.find_one({
                "user_id": str(current_user.id),
                "scenario_id": scenario_id
            })
            
            if not assignment:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail="You are not assigned to this scenario"
                )
            
            # Check if mode is assigned
            assigned_modes = assignment.get("assigned_modes", [])
            if mode not in assigned_modes:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail=f"You are not assigned to the {mode} of this scenario"
                )
    
    # Initialize chat session
    session = await initialize_chat_session(db, avatar_interaction_id=avatar_interaction_id, mode=mode, persona_id=persona_id,avatar_id=avatar_id, language_id=language_id,current_user=current_user.id)
    
    # Return session information
    return {
        "id":session.id,
        "scenario_name": session.scenario_name,
        "avatar_interaction_id": str(avatar_interaction_id),
        "mode": mode,
        "persona_id": str(persona_id) if persona_id else None,
        "avatar_id": str(avatar_id) if avatar_id else None,
        "language_id": str(language_id) if language_id else None,
        "created_at": session.created_at
    }



@router.post("/chat")
async def chat(
    message: str = Form(...),
    id: str = Form(...),
    name: Optional[str] = Form(None),
    db: Any = Depends(get_db)
):
    """
    Send a message to the chat system and prepare for streaming response.
    """
    # Get existing session
    session = await get_chat_session(db, id)
    if not session:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Session not found"
        )
    
    # Get the chat factory
    chat_factory = await get_chat_factory()
    
    # Extract avatar_interaction_id from session
    avatar_interaction_id = session.avatar_interaction
    
    # If persona_id is in the request, we'll use it, otherwise check if it's in the session
    persona_id = None
    if hasattr(session, "persona_id") and session.persona_id:
        persona_id = session.persona_id
        # print("self.config.persona_idsss",persona_id)
    # Get the appropriate chat handler - use the mode from the avatar_interaction document
    # You'll need to fetch the avatar_interaction to determine the mode
    persona_id=await db.avatars.find_one({"_id": session.avatar_id})
    # print(persona_id["_id"],"self.config.persona_idasdasdsd")
    avatar_interaction = await db.avatar_interactions.find_one({"_id": avatar_interaction_id})
    mode = avatar_interaction["mode"] if avatar_interaction else "try_mode"  # Default to try_mode
    language_id=session.language_id
    # print(persona_id,"self.config.persona_idsss")
    handler = await chat_factory.get_chat_handler(
        avatar_interaction_id=UUID(avatar_interaction_id),
        mode=mode,
        persona_id=UUID(persona_id["_id"]) if persona_id else None,
        language_id=language_id 
    )
    
    # Add user message to conversation history
    user_message = Message(
        role=handler.config.bot_role_alt,
        content=message,
        timestamp=datetime.now()
    )
    
    # Initialize conversation_history if it doesn't exist
    if not hasattr(session, "conversation_history") or session.conversation_history is None:
        session.conversation_history = []
    
    session.conversation_history.append(user_message)
    
    # Update session in database
    await update_chat_session(db, session)
    
    # Return a simple acknowledgment
    return {
        "message": "Message received, processing...",
        "id": id
    }




@router.get("/chat/stream")
async def chat_stream(
    id: str = Query(...),
    name: Optional[str] = Query(None),
    db: Any = Depends(get_db),
    current_user: UserDB = Depends(get_current_user)  # Add this dependency
):
    """
    Stream the response for a chat message.
    """
    # Get session
    session = await get_chat_session(db, id)
    if not session:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Session not found")
    
    # Get the chat factory
    chat_factory = await get_chat_factory()
    
    # Extract avatar_interaction_id and mode from session
    avatar_interaction_id = session.avatar_interaction
    
    # Fetch avatar_interaction to get mode
    avatar_interaction = await db.avatar_interactions.find_one({"_id": avatar_interaction_id})
    
    mode = avatar_interaction["mode"] if avatar_interaction else "try_mode"  # Default to try_mode
    print("avatar_interaction",mode)
    # Extract persona_id if available
    avatar = await db.avatars.find_one({"_id": session.avatar_id})
    if not avatar:
        raise HTTPException(status_code=404, detail="Avatar not found")
    persona_id = avatar["persona_id"][0]
    language_id = session.language_id
    
    # Get the appropriate chat handler
    handler = await chat_factory.get_chat_handler(
        avatar_interaction_id=UUID(avatar_interaction_id),
        mode=mode,
        persona_id=UUID(persona_id) if persona_id else None,
        language_id=language_id 
    )
    # await handler.initialize_fact_checking(id)
    # Get template data for coaching rules
    try:
        # Get scenario to find template_id
        session = await get_chat_session(db, id)
        if not session or not session.avatar_interaction:
            print("Warning: No session or avatar_interaction found")
            await handler.initialize_fact_checking(id, coaching_rules={})
            return
        avatar_interaction = await db.avatar_interactions.find_one({"_id": str(session.avatar_interaction)})
        if not avatar_interaction:
            print("Warning: Avatar interaction not found")
            await handler.initialize_fact_checking(id, coaching_rules={})
            return
        # Find scenario with this avatar interaction
        scenario = None
        avatar_interaction_id = session.avatar_interaction
        avatar_interaction_str = str(session.avatar_interaction)
        try:
            avatar_interaction_uuid = UUID(avatar_interaction_str)
        except ValueError:
            avatar_interaction_uuid = None
        print(f"üîç Looking for scenario with avatar_interaction: {avatar_interaction_str}")
        query_conditions = []
        for mode_type in ["learn_mode", "try_mode", "assess_mode"]:
            query_conditions.extend([
            {f"{mode_type}.avatar_interaction": avatar_interaction_str},
            {f"{mode_type}.avatar_interaction": avatar_interaction_uuid} if avatar_interaction_uuid else {}
        ])
            query_conditions = [q for q in query_conditions if q]
            scenario = await db.scenarios.find_one({"$or": query_conditions})
            print(f"üîç Scenario found: {scenario is not None}")
            if scenario:
                print(f"‚úÖ Found scenario: {scenario.get('_id')}")
        # Get template data
        coaching_rules = {}
        print("scenario.get",scenario.get("template_id"))
        if scenario and scenario.get("template_id"):
            print("step1",scenario.get("template_id"))
            template = await db.templates.find_one({"id": scenario["template_id"]})
            # print("step2",template)
            if template and template.get("template_data"):
                coaching_rules = template["template_data"].get("coaching_rules", {})
                # print("step3",coaching_rules)
    
        await handler.initialize_fact_checking(id, coaching_rules=coaching_rules)
    
    except Exception as e:
        print(f"Warning: Could not load coaching rules: {e}")
        await handler.initialize_fact_checking(id, coaching_rules={})       
    # Get the most recent user message
    if not session.conversation_history:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No conversation history found")
    
    previous_message = session.conversation_history[-1]
    message = previous_message.content
    
    # Process the message and get the response stream
    try:
        response = await handler.process_message(
            message,
            session.conversation_history,
            name
        )
        
        async def stream_chat():
            res = ""
            async for chunk in response:
                updated_message = chunk["chunk"]
                
                # Check if complete
                if chunk["finish"] == "stop" and chunk["usage"] is not None:
                    # Add bot message to conversation history
                    bot_message = Message(
                        role=handler.config.bot_role,
                        content=updated_message,
                        timestamp=datetime.now()
                    )
                    bot_message.usage = chunk["usage"]
                    if chunk.get("fact_check_summary"):
                        bot_message.metadata = {
                            "fact_check_summary": chunk["fact_check_summary"]
                        }
                                        
                    session.conversation_history.append(bot_message)
                    await update_chat_session(db, session)
                
                # Parse for correct formatting tags
                result = re.split(r"\[CORRECT\]", updated_message)
                correct_answer = ''
                if len(result) >= 3:
                    correct_answer = result[1]
                    answer = result[0]
                else:
                    emotion = "neutral"  # Default emotion if parsing fails
                    answer = re.sub(r"\[CORRECT\]","",updated_message)
                
                # Check if this is the end of the conversation
                is_finished = "[FINISH]" in updated_message
                if is_finished:
                    answer = updated_message.replace("[FINISH]", " ")
                    complete = True
                    
                    # Update mode completion
                    try:
                        # Find the scenario that has this avatar interaction
                        scenario_query = {}
                        
                        if mode == "learn_mode":
                            scenario_query = {"learn_mode.avatar_interaction": avatar_interaction_id}
                        elif mode == "try_mode":
                            scenario_query = {"try_mode.avatar_interaction": avatar_interaction_id}
                        elif mode == "assess_mode":
                            scenario_query = {"assess_mode.avatar_interaction": avatar_interaction_id}
                        
                        scenarios = await db.scenarios.find(scenario_query).to_list(length=1)
                        
                        if scenarios:
                            scenario_id = scenarios[0]["_id"]
                            
                            # Update mode completion
                            await db.user_scenario_assignments.update_one(
                                {
                                    "user_id": str(current_user.id),
                                    "scenario_id": scenario_id
                                },
                                {
                                    "$set": {
                                        f"mode_progress.{mode}.completed": True,
                                        f"mode_progress.{mode}.completed_date": datetime.now()
                                    }
                                }
                            )
                            
                            # Check if all assigned modes are now completed
                            assignment = await db.user_scenario_assignments.find_one({
                                "user_id": str(current_user.id),
                                "scenario_id": scenario_id
                            })
                            
                            if assignment:
                                # Check if all modes are completed
                                assigned_modes = assignment.get("assigned_modes", [])
                                mode_progress = assignment.get("mode_progress", {})
                                
                                all_modes_completed = True
                                for assigned_mode in assigned_modes:
                                    if assigned_mode not in mode_progress or not mode_progress.get(assigned_mode, {}).get("completed", False):
                                        all_modes_completed = False
                                        break
                                
                                # If all modes are completed, mark scenario as completed
                                if all_modes_completed:
                                    await db.user_scenario_assignments.update_one(
                                        {
                                            "user_id": str(current_user.id),
                                            "scenario_id": scenario_id
                                        },
                                        {
                                            "$set": {
                                                "completed": True,
                                                "completed_date": datetime.now()
                                            }
                                        }
                                    )
                                    
                                    # Update module completion
                                    from core.module_assignment import update_module_completion_status
                                    await update_module_completion_status(
                                        db, 
                                        current_user.id, 
                                        UUID(assignment.get("module_id"))
                                    )
                    except Exception as e:
                        print(f"Error updating mode completion: {e}")
                else:
                    complete = False
                
                # Check if correction is needed
                if "[CORRECT]" in updated_message:
                    correct = False
                else:
                    correct = True
                
                # Format the response as a ChatResponse
                chat_response = ChatResponse(
                    id=session.id,
                    response=answer,
                    emotion=emotion if 'emotion' in locals() else "neutral",
                    complete=complete,
                    correct=correct,
                    correct_answer=correct_answer,
                    conversation_history=session.conversation_history
                )
                response_data = {
    "response": chat_response.response,
    "emotion": getattr(chat_response, 'emotion', 'neutral'),
    "complete": chat_response.complete,
    "correct": getattr(chat_response, 'correct', True),
    "correct_answer": getattr(chat_response, 'correct_answer', ''),
    "fact_check_summary": getattr(chat_response, 'fact_check_summary', None),
    "finish": "stop" if chat_response.complete else None  # Add this for frontend
}

                if chunk.get("fact_check_summary"):
                    print("chunk.get("")",chunk.get("fact_check_summary"))
                    chat_response.fact_check_summary = chunk["fact_check_summary"]                
                # yield f"data: {chat_response.json()}\n\n"
                yield f"data: {json.dumps(response_data)}\n\n"

                
        return StreamingResponse(stream_chat(), media_type="text/event-stream")
        
    except Exception as e:
        print(f"Error in chat stream: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
@router.get("/chat/history/{id}")
async def get_chat_history(
    id: str,
    db: Any = Depends(get_db)
):
    """
    Get the conversation history for a chat session.
    
    - id: ID of the chat session
    """
    # Get session
    session = await get_chat_session(db, id)
    if not session:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Session not found")
    
    return {
        "id":session.id,
        "scenario_name": session.scenario_name,
        # "persona_id": session.persona_id,
        "conversation_history": [msg.dict() for msg in session.conversation_history],
        "created_at": session.created_at,
        "last_updated": session.last_updated
    }


@router.put("/chat/persona/{id}")
async def update_session_persona(
    id: str,
    persona_id: UUID = Body(...),
    db: Any = Depends(get_db)
):
    """
    Update the persona used in a chat session.
    
    - id: ID of the chat session
    - persona_id: ID of the new persona to use
    """
    # Get session
    session = await get_chat_session(db, id)
    if not session:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Session not found")
    
    # Verify persona exists
    persona = await db.personas.find_one({"_id": str(persona_id)})
    if not persona:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Persona not found")
    
    # Update session with new persona
    session.persona_id = str(persona_id)
    await update_chat_session(db, session)
    
    return {
        "id":session.id,
        "persona_id": str(persona_id),
        "message": "Persona updated successfully"
    }


@router.delete("/chat/session/{id}")
async def delete_chat_session(
    id: str,
    db: Any = Depends(get_db)
):
    """
    Delete a chat session.
    
    - id: ID of the chat session to delete
    """
    # Verify session exists
    session = await get_chat_session(db, id)
    if not session:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Session not found")
    
    # Delete the session
    result = await db.sessions.delete_one({"id": id})
    
    return {
        "success": result.deleted_count > 0,
        "id": id
    }

from uuid import uuid4
import json

@router.post("/chat/evaluate/{session_id}")
async def evaluate_conversation(
    session_id: str,
    db: Any = Depends(get_db),
    current_user: UserDB = Depends(get_current_user)
):
    """Generate assessment report and update assignment progress"""
    
    try:
        # Get session
        session = await get_chat_session(db, session_id)
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # Get avatar interaction and scenario
        avatar_interaction_id = session.avatar_interaction
        avatar_interaction = await db.avatar_interactions.find_one({"_id": avatar_interaction_id})
        
        if not avatar_interaction:
            raise HTTPException(status_code=404, detail="Avatar interaction not found")
        
        # Find the scenario
        scenario_query = {}
        mode = "assess_mode"  # Default
        
        # Check which mode this avatar interaction belongs to
        for mode_type in ["learn_mode", "try_mode", "assess_mode"]:
            scenarios = await db.scenarios.find({f"{mode_type}.avatar_interaction": avatar_interaction_id}).to_list(length=1)
            if scenarios:
                scenario = scenarios[0]
                mode = mode_type
                break
        else:
            raise HTTPException(status_code=404, detail="Scenario not found")
        
        # Get template data for evaluation criteria
        template_id = scenario.get("template_id")
        template = await db.templates.find_one({"id": template_id}) if template_id else None
        knowledge_base_id = template.get("knowledge_base_id")    
        # Generate assessment report
        evaluation_result = await analyze_conversation_with_factcheck(
            session.conversation_history,
            template.get("evaluation_metrics", {}) if template else {},
            knowledge_base_id
        )
        
        # Store evaluation result
        evaluation_record = {
            "_id": str(uuid4()),
            "session_id": session_id,
            "user_id": str(current_user.id),
            "scenario_id": scenario["_id"],
            "mode": mode,
            "evaluation_result": evaluation_result,
            "evaluated_at": datetime.now(),
            "evaluated_by": str(current_user.id)
        }
        
        await db.analysis.insert_one(evaluation_record)
        
        # Update assignment with evaluation data
        assignment_updated = False
        try:
            result = await db.user_scenario_assignments.update_one(
                {
                    "user_id": str(current_user.id),
                    "scenario_id": scenario["_id"]
                },
                {
                    "$set": {
                        f"mode_progress.{mode}.evaluation_completed": True,
                        f"mode_progress.{mode}.evaluation_date": datetime.now(),
                        f"mode_progress.{mode}.latest_score": evaluation_result.get("overall_score", 0),
                        f"mode_progress.{mode}.performance_category": evaluation_result.get("performance_category", "Needs Assessment"),
                        f"mode_progress.{mode}.evaluation_id": evaluation_record["_id"]
                    }
                }
            )
            assignment_updated = result.modified_count > 0
        except Exception as e:
            print(f"Error updating assignment: {e}")
        
        # Update module completion if applicable
        try:
            from core.module_assignment import update_module_completion_status
            # Find module that contains this scenario
            modules = await db.modules.find({"scenarios": scenario["_id"]}).to_list(length=1)
            if modules:
                module_id = modules[0]["_id"]
                await update_module_completion_status(db, current_user.id, UUID(module_id))
        except Exception as e:
            print(f"Error updating module completion: {e}")
        
        return {
            "session_id": session_id,
            "evaluation": evaluation_result,
            "evaluation_id": evaluation_record["_id"],
            "assignment_updated": assignment_updated,
            "score": evaluation_result.get("overall_score", 0),
            "mode": mode
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in evaluate_conversation: {e}")
        raise HTTPException(status_code=500, detail=f"Evaluation failed: {str(e)}")


# async def analyze_conversation_with_factcheck(
#     conversation_history: List[Message],
#     evaluation_metrics: Dict[str, Any],
#     knowledge_base_id: Optional[str]
# ) -> Dict[str, Any]:
#     """Single prompt analysis like your old method + fact-checking"""
    
#     try:
#         from openai import AsyncAzureOpenAI
#         import os
        
#         openai_client = AsyncAzureOpenAI(
#             api_key=os.getenv("api_key"),
#             azure_endpoint=os.getenv("endpoint"),
#             api_version=os.getenv("api_version")
#         )
        
#         # Build conversation text
#         conversation_text = ""
#         for msg in conversation_history:
#             role = "Learner" if msg.role != "assistant" else "Customer"
#             conversation_text += f"{role}: {msg.content}\n"
        
#         # Get relevant knowledge for fact-checking (if available)
#         knowledge_context = ""
#         if knowledge_base_id:
#             # Simple search for key terms in conversation
#             from core.azure_search_manager import AzureVectorSearchManager
#             vector_search = AzureVectorSearchManager()
            
#             search_results = await vector_search.vector_search(
#                 conversation_text[:500], knowledge_base_id, top_k=5, openai_client=openai_client
#             )
            
#             if search_results:
#                 knowledge_context = "OFFICIAL INFORMATION FOR FACT-CHECKING:\n"
#                 for result in search_results:
#                     knowledge_context += f"- {result['content']}\n"
        
#         # Single evaluation prompt (like your old method)
#         evaluation_prompt = f"""
#         Analyze this customer service conversation and provide a comprehensive assessment.
        
#         CONVERSATION:
#         {conversation_text}
        
#         {knowledge_context}
        
#         EVALUATION CRITERIA:
#         {json.dumps(evaluation_metrics, indent=2) if evaluation_metrics else "Use standard customer service metrics"}
        
#         Provide assessment in JSON format:
#         {{
#             "overall_score": 0-100,
#             "performance_category": "Excellent/Good/Satisfactory/Needs Improvement",
#             "factual_accuracy": 0-100,
#             "metric_scores": {{
#                 "knowledge_accuracy": 0-100,
#                 "communication_quality": 0-100,
#                 "customer_service_skills": 0-100,
#                 "professionalism": 0-100
#             }},
#             "strengths": ["strength 1", "strength 2"],
#             "areas_for_improvement": ["area 1", "area 2"],
#             "recommendations": ["recommendation 1", "recommendation 2"],
#             "fact_check_summary": "Summary of any factual errors found",
#             "conversation_summary": "Brief summary of what happened"
#         }}
        
#         If official information is provided, check learner responses for factual accuracy.
#         Focus on practical, actionable feedback.
#         """
        
#         response = await openai_client.chat.completions.create(
#             model="gpt-4o",
#             messages=[
#                 {"role": "system", "content": "You are an expert training evaluator who provides comprehensive but concise assessments."},
#                 {"role": "user", "content": evaluation_prompt}
#             ],
#             temperature=0.2,
#             max_tokens=1000
#         )
        
#         # result = json.loads(response.choices[0].message.content)
     
#         result_text = response.choices[0].message.content.strip()
#         print(f"üîç LLM Response: {result_text[:200]}...")  # Debug log
        
#         # Try to extract JSON from response
#         try:
#             # First try direct parsing
#             result = json.loads(result_text)
#         except json.JSONDecodeError:
#             # Try to extract JSON from markdown blocks
#             json_match = re.search(r'```json\s*(.*?)\s*```', result_text, re.DOTALL)
#             if json_match:
#                 result = json.loads(json_match.group(1))
#             else:
#                 # Fallback if JSON parsing fails
#                 print(f"‚ùå JSON parsing failed. Raw response: {result_text}")
#                 result = {
#                     "overall_score": 50,
#                     "performance_category": "Needs Assessment",
#                     "factual_accuracy": 50,
#                     "metric_scores": {
#                         "knowledge_accuracy": 50,
#                         "communication_quality": 50,
#                         "customer_service_skills": 50,
#                         "professionalism": 50
#                     },
#                     "strengths": ["Assessment could not be completed"],
#                     "areas_for_improvement": ["Please try evaluation again"],
#                     "recommendations": ["Review conversation and retry assessment"],
#                     "fact_check_summary": "Could not analyze due to parsing error",
#                     "conversation_summary": "Analysis incomplete",
#                     "parsing_error": True,
#                     "raw_response": result_text[:500]
#                 }        
#         # Add metadata
#         result["evaluation_method"] = "single_prompt_with_factcheck"
#         result["knowledge_base_used"] = knowledge_base_id is not None
#         result["conversation_length"] = len(conversation_history)
        
#         return result
        
#     except Exception as e:
#         print(f"Error in conversation analysis: {e}")
#         return {
#             "overall_score": 0,
#             "error": f"Analysis failed: {str(e)}",
#             "conversation_length": len(conversation_history)
#         }
async def analyze_conversation_with_factcheck(
    conversation_history: List[Message],
    evaluation_metrics: Dict[str, Any],
    knowledge_base_id: Optional[str]
) -> Dict[str, Any]:
    """Single prompt analysis + enhanced fact-checking with template coaching rules"""
    
    try:
        from openai import AsyncAzureOpenAI
        import os
        
        openai_client = AsyncAzureOpenAI(
            api_key=os.getenv("api_key"),
            azure_endpoint=os.getenv("endpoint"),
            api_version=os.getenv("api_version")
        )
        
        # Build conversation text (your existing code)
        conversation_text = ""
        for msg in conversation_history:
            role = "Learner" if msg.role != "assistant" else "Customer"
            conversation_text += f"{role}: {msg.content}\n"
        
        # Get relevant knowledge for fact-checking (your existing code)
        knowledge_context = ""
        fact_check_results = []  # ‚úÖ ADD: Store detailed fact-check results
        
        if knowledge_base_id:
            from core.azure_search_manager import AzureVectorSearchManager, EnhancedFactChecker
            vector_search = AzureVectorSearchManager()
            
            # ‚úÖ ENHANCED: Initialize fact-checker with empty coaching rules for evaluation
            # (We're not doing real-time coaching here, just thorough analysis)
            fact_checker = EnhancedFactChecker(vector_search, openai_client, coaching_rules={})
            
            # ‚úÖ ADD: Detailed fact-checking of learner responses
            try:
                # Get learner responses (not customer responses)
                learner_responses = [
                    msg.content for msg in conversation_history 
                    if msg.role != "assistant"  # assistant = customer, others = learner
                ]
                
                # Fact-check the last few learner responses
                for response in learner_responses[-3:]:  # Check last 3 learner responses
                    if len(response.strip()) > 10:  # Skip very short responses
                        verifications = await fact_checker.verify_response_claims(
                            response, 
                            knowledge_base_id, 
                            conversation_history=conversation_history  # ‚úÖ Pass conversation context
                        )
                        
                        # Store detailed results
                        for verification in verifications:
                            fact_check_results.append({
                                "claim": verification.claim.get("claim_text", ""),
                                "result": verification.result.value,
                                "confidence": verification.confidence_score,
                                "explanation": verification.explanation,
                                "coaching_feedback": getattr(verification, 'coaching_feedback', None)
                            })
                        
            except Exception as fact_error:
                print(f"Detailed fact-checking error: {fact_error}")
            
            # Your existing search logic (keep this)
            search_results = await vector_search.vector_search(
                conversation_text[:500], knowledge_base_id, top_k=5, openai_client=openai_client
            )
            
            if search_results:
                knowledge_context = "OFFICIAL INFORMATION FOR FACT-CHECKING:\n"
                for result in search_results:
                    knowledge_context += f"- {result['content']}\n"
        
        # ‚úÖ ENHANCED: Build fact-check summary for evaluation prompt
        fact_check_summary = ""
        if fact_check_results:
            fact_errors = [fc for fc in fact_check_results if fc["result"] in ["INCORRECT", "PARTIALLY_CORRECT"]]
            fact_check_summary = f"""
DETAILED FACT-CHECK RESULTS:
- Total claims analyzed: {len(fact_check_results)}
- Factual errors found: {len(fact_errors)}
- Key issues: {[fc["explanation"] for fc in fact_errors[:3]]}
"""
        
        # ‚úÖ ENHANCED: Evaluation prompt with fact-check integration
        evaluation_prompt = f"""
        Analyze this customer service conversation and provide a comprehensive assessment.
        
        CONVERSATION:
        {conversation_text}
        
        {knowledge_context}
        
        {fact_check_summary}
        
        EVALUATION CRITERIA:
        {json.dumps(evaluation_metrics, indent=2) if evaluation_metrics else "Use standard customer service metrics"}
        
        Provide assessment in JSON format:
        {{
            "overall_score": 0-100,
            "performance_category": "Excellent/Good/Satisfactory/Needs Improvement",
            "factual_accuracy": 0-100,
            "metric_scores": {{
                "knowledge_accuracy": 0-100,
                "communication_quality": 0-100,
                "customer_service_skills": 0-100,
                "professionalism": 0-100
            }},
            "strengths": ["strength 1", "strength 2"],
            "areas_for_improvement": ["area 1", "area 2"],
            "recommendations": ["recommendation 1", "recommendation 2"],
            "fact_check_summary": "Summary of any factual errors found",
            "conversation_summary": "Brief summary of what happened"
        }}
        
        IMPORTANT: 
        - Use the detailed fact-check results above to assess factual accuracy
        - If factual errors were found, reflect this in the knowledge_accuracy score
        - Include specific fact-checking findings in your recommendations
        - Focus on practical, actionable feedback
        """
        
        # Your existing LLM call (keep this)
        response = await openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an expert training evaluator who provides comprehensive but concise assessments."},
                {"role": "user", "content": evaluation_prompt}
            ],
            temperature=0.2,
            max_tokens=1000
        )
        
        # Your existing JSON parsing (keep this)
        result_text = response.choices[0].message.content.strip()
        print(f"üîç LLM Response: {result_text[:200]}...")
        
        try:
            result = json.loads(result_text)
        except json.JSONDecodeError:
            json_match = re.search(r'```json\s*(.*?)\s*```', result_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                # Your existing fallback (keep this)
                result = {
                    "overall_score": 50,
                    "performance_category": "Needs Assessment",
                    "factual_accuracy": 50,
                    "metric_scores": {
                        "knowledge_accuracy": 50,
                        "communication_quality": 50,
                        "customer_service_skills": 50,
                        "professionalism": 50
                    },
                    "strengths": ["Assessment could not be completed"],
                    "areas_for_improvement": ["Please try evaluation again"],
                    "recommendations": ["Review conversation and retry assessment"],
                    "fact_check_summary": "Could not analyze due to parsing error",
                    "conversation_summary": "Analysis incomplete",
                    "parsing_error": True,
                    "raw_response": result_text[:500]
                }
        
        # ‚úÖ ENHANCED: Add detailed fact-check data to result
        result["evaluation_method"] = "single_prompt_with_enhanced_factcheck"
        result["knowledge_base_used"] = knowledge_base_id is not None
        result["conversation_length"] = len(conversation_history)
        result["detailed_fact_checks"] = fact_check_results  # ‚úÖ ADD: Detailed fact-check results
        result["fact_check_stats"] = {
            "total_claims": len(fact_check_results),
            "factual_errors": len([fc for fc in fact_check_results if fc["result"] in ["INCORRECT", "PARTIALLY_CORRECT"]]),
            "accuracy_percentage": round(
                (len([fc for fc in fact_check_results if fc["result"] == "CORRECT"]) / len(fact_check_results) * 100) 
                if fact_check_results else 100, 2
            )
        }
        
        return result
        
    except Exception as e:
        print(f"Error in conversation analysis: {e}")
        return {
            "overall_score": 0,
            "error": f"Analysis failed: {str(e)}",
            "conversation_length": len(conversation_history)
        }